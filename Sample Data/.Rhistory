length(edgeVal[[1]])
choose(50,2)
85/5
D = NULL
for (j in 1:2){
DpopJ = vector("list", subNum)
for (k in 1:subNum){
DpopJ[[k]] = mvrnorm(n = n, mu = rep(0, p), P[[j]])
}
D = append(D, DpopJ)
}
D = lapply(D, scale)
### now run various models & store results:
# --- MoG model ---
lamMOG= regChoice(D=D, lambda = c(0.1, 0.125, .15), K=c(1,2,3), SampleNo = 10, Ptype="Classic")
lamMOG|
lamMOG
lamMOG= regChoice(D=D, lambda = c(0.15, .2), K=c(1,2,3), SampleNo = 10, Ptype="Classic")
lamMOG
dim(D[[1]])
lamMOG= regChoice(D=D, lambda = c(.3, .4, .5), K=c(1,2,3), SampleNo = 10, Ptype="Classic")
lamMOG
lamMOG= regChoice(D=D, lambda = c(.05, .1), K=c(1,2,3), SampleNo = 10, Ptype="Classic")
lamMOG
lamMOG= regChoice(D=D, lambda = c(.05), K=c(1,2,3), SampleNo = 10, Ptype="Classic")
lamMOG
sharedComps = 0
set.seed(i)
randomNetworks = gen2scaleFree(compNum = compNum, compNode = compNode, sharedComps = sharedComps)#, m = 3)
P = vector("list", 2)
P[[1]] = randomNetworks$A
P[[2]] = randomNetworks$B
#     edgeList = lapply(P, FUN=function(x){
#       which(x[upper.tri(x)]!=0)
#     })
#
D = NULL
for (j in 1:2){
DpopJ = vector("list", subNum)
for (k in 1:subNum){
DpopJ[[k]] = mvrnorm(n = n, mu = rep(0, p), P[[j]])
}
D = append(D, DpopJ)
}
D = lapply(D, scale)
### now run various models & store results:
lamMOG= regChoice(D=D, lambda = c(.05), K=c(1,2,3), SampleNo = 10, Ptype="Classic")
lamMOG
lamMOG= regChoice(D=D, lambda = c(.1, .2), K=c(1,2,3), SampleNo = 10, Ptype="Classic")
lamMOG
X = PenVanillaEM(D = D, K = lamMOG$Kval, lambda = lamMOG$lamVal, Ptype = "Classic", tol = 0.001)
if (lamMOG$Kval>1){
iter = 0
while(X$iter==0 & iter < 5){
#       print("hi")
X = PenVanillaEM(D = D, K = lamMOG$Kval, lambda = lamMOG$lamVal, Ptype = "Classic", tol = 0.001)
iter = iter+1
}
}
trueClust = c(rep(1,subNum), rep(2, subNum))
trueEdges = c()
reportedEdges = c()
for (k in 1:length(X$SubjectTheta)){
# get estimated cluster for kth subject:
P_Est_k = X$SubjectTheta[[k]]#X$Theta[,, which.max(X$Z[k,])]
#     getPerformance(Est = P_Est_k, True = P[[trueClust[k]]])
trueEdges = c(trueEdges, 1*(P[[trueClust[k]]][upper.tri(P_Est_k)]!=0))
reportedEdges = c(reportedEdges, 1*(P_Est_k[upper.tri(P_Est_k)]!=0))
}
res = HMeasure(true.class = trueEdges, scores = reportedEdges )
ResultsMoG$SimID[i] = i
ResultsMoG$TP[i] = res$metrics$TP
ResultsMoG$FP[i] = res$metrics$FP
ResultsMoG$Sens[i] = res$metrics$TPR
ResultsMoG$Spec[i] = res$metrics$TN/(res$metrics$FP + res$metrics$TN)
ResultsMoG$FDR[i] = res$metrics$FPR
ResultsMoG$TPR[i] = res$metrics$TPR
ResultsMoG$AUC[i] = res$metrics$AUC
meanKL = 0
for (j in 1:length(D)){
meanKL = meanKL + KLDist(SigmaTrue = P[[trueClust[j]]], PresEst = X$SubjectTheta[[j]])
}
ResultsMoG$meanKL[i] = (meanKL)/length(D)
ResultsMoG$Kchoice[i] = lamMOG$Kval
library(hmeasure)
library(hmeasure)
trueClust = c(rep(1,subNum), rep(2, subNum))
trueEdges = c()
reportedEdges = c()
for (k in 1:length(X$SubjectTheta)){
# get estimated cluster for kth subject:
P_Est_k = X$SubjectTheta[[k]]#X$Theta[,, which.max(X$Z[k,])]
#     getPerformance(Est = P_Est_k, True = P[[trueClust[k]]])
trueEdges = c(trueEdges, 1*(P[[trueClust[k]]][upper.tri(P_Est_k)]!=0))
reportedEdges = c(reportedEdges, 1*(P_Est_k[upper.tri(P_Est_k)]!=0))
}
res = HMeasure(true.class = trueEdges, scores = reportedEdges )
ResultsMoG$SimID[i] = i
ResultsMoG$TP[i] = res$metrics$TP
ResultsMoG$FP[i] = res$metrics$FP
ResultsMoG$Sens[i] = res$metrics$TPR
ResultsMoG$Spec[i] = res$metrics$TN/(res$metrics$FP + res$metrics$TN)
ResultsMoG$FDR[i] = res$metrics$FPR
ResultsMoG$TPR[i] = res$metrics$TPR
ResultsMoG$AUC[i] = res$metrics$AUC
meanKL = 0
for (j in 1:length(D)){
meanKL = meanKL + KLDist(SigmaTrue = P[[trueClust[j]]], PresEst = X$SubjectTheta[[j]])
}
ResultsMoG$meanKL[i] = (meanKL)/length(D)
ResultsMoG$Kchoice[i] = lamMOG$Kval
ResultsMoG[1,]
lamMOG= regChoice(D=D, lambda = c(.1, .12, .15), K=c(1,2,3), SampleNo = 10, Ptype="Classic")
lamMOG
regChoice(D=D, lambda = c(.1, .15, .2), K=c(1,2,3), SampleNo = 10, Ptype="Classic")
lamMOG= regChoice(D=D, lambda = c(.15, .2), K=c(1,2,3), SampleNo = 10, Ptype="Classic")
lamMOG
X = PenVanillaEM(D = D, K = lamMOG$Kval, lambda = lamMOG$lamVal, Ptype = "Classic", tol = 0.001)
if (lamMOG$Kval>1){
iter = 0
while(X$iter==0 & iter < 5){
#       print("hi")
X = PenVanillaEM(D = D, K = lamMOG$Kval, lambda = lamMOG$lamVal, Ptype = "Classic", tol = 0.001)
iter = iter+1
}
}
X$C
trueClust = c(rep(1,subNum), rep(2, subNum))
trueEdges = c()
reportedEdges = c()
for (k in 1:length(X$SubjectTheta)){
# get estimated cluster for kth subject:
P_Est_k = X$SubjectTheta[[k]]#X$Theta[,, which.max(X$Z[k,])]
#     getPerformance(Est = P_Est_k, True = P[[trueClust[k]]])
trueEdges = c(trueEdges, 1*(P[[trueClust[k]]][upper.tri(P_Est_k)]!=0))
reportedEdges = c(reportedEdges, 1*(P_Est_k[upper.tri(P_Est_k)]!=0))
}
res = HMeasure(true.class = trueEdges, scores = reportedEdges )
ResultsMoG$SimID[i] = i
ResultsMoG$TP[i] = res$metrics$TP
ResultsMoG$FP[i] = res$metrics$FP
ResultsMoG$Sens[i] = res$metrics$TPR
ResultsMoG$Spec[i] = res$metrics$TN/(res$metrics$FP + res$metrics$TN)
ResultsMoG$FDR[i] = res$metrics$FPR
ResultsMoG$TPR[i] = res$metrics$TPR
ResultsMoG$AUC[i] = res$metrics$AUC
meanKL = 0
for (j in 1:length(D)){
meanKL = meanKL + KLDist(SigmaTrue = P[[trueClust[j]]], PresEst = X$SubjectTheta[[j]])
}
ResultsMoG$meanKL[i] = (meanKL)/length(D)
ResultsMoG$Kchoice[i] = lamMOG$Kval
ResultsMoG[1,]
D_FLG = array(0, c(n,p,2*subNum))
for (k in 1:(2*subNum)){
D_FLG[,,k] = D[[k]]
}
lamFGL = regChoiceJGL(D = D_FLG, l1=c(.05, .1), l2 = c(.2, .3))
X = FGL(data = D_FLG, l1 = lamFGL$l1, l2 = lamFGL$l2, tol = 0.001)
trueEdges = c()
reportedEdges = c()
for (k in 1:(2*subNum)){
# get estimated cluster for kth subject:
P_Est_k = X$Z[,,k]
trueEdges = c(trueEdges, 1*(P[[trueClust[k]]][upper.tri(P_Est_k)]!=0))
reportedEdges = c(reportedEdges, 1*(P_Est_k[upper.tri(P_Est_k)]!=0))
#
}
res = HMeasure(true.class = trueEdges, scores = reportedEdges )
ResultsFGL$SimID[i] = i
ResultsFGL$TP[i] = res$metrics$TP
ResultsFGL$FP[i] = res$metrics$FP
ResultsFGL$Sens[i] = res$metrics$TPR
ResultsFGL$Spec[i] = res$metrics$TN/(res$metrics$FP + res$metrics$TN)
ResultsFGL$FDR[i] = res$metrics$FPR
ResultsFGL$TPR[i] = res$metrics$TPR
ResultsFGL$AUC[i] = res$metrics$AUC
meanKL = 0
for (j in 1:length(D)){
meanKL = meanKL + KLDist(SigmaTrue = P[[trueClust[j]]], PresEst = X$Z[,,j])
#       meanKL = meanKL + KLDist(SigmaTrue = P[[trueClust[j]]], PresEst = X$theta[[j]])
}
ResultsFGL$meanKL[i] = (meanKL)/length(D)
ResultsFGL[1,]
n = 75
rm(list=ls())
gc()
source('/media/1401-1FFE/Documents/Model Based Clustering/Simulations/Comparison Sims/Simulation_BA_VaryingSim_01.R')
ResultsMoG = data.frame(matrix(0, ncol=12, nrow=Nsims))
names(ResultsMoG) = c("SimID", "TP", "FP", "Sens", "Spec", "FDR", "TPR", "Pres", "Recall", "AUC", "meanKL", "Kchoice")
ResultsFGL= data.frame(matrix(0, ncol=11, nrow=Nsims))
names(ResultsFGL) = c("SimID", "TP", "FP", "Sens", "Spec", "FDR", "TPR", "Pres", "Recall", "AUC", "meanKL")
ResultsGGL= data.frame(matrix(0, ncol=11, nrow=Nsims))
names(ResultsGGL) = c("SimID", "TP", "FP", "Sens", "Spec", "FDR", "TPR", "Pres", "Recall", "AUC", "meanKL")
ResultsGL= data.frame(matrix(0, ncol=11, nrow=Nsims))
names(ResultsGL) = c("SimID", "TP", "FP", "Sens", "Spec", "FDR", "TPR", "Pres", "Recall", "AUC", "meanKL")
p = compNode * compNum # number of nodes
# start running simulations:
set.seed(i)
randomNetworks = gen2scaleFree(compNum = compNum, compNode = compNode, sharedComps = sharedComps)#, m = 3)
P = vector("list", 2)
P[[1]] = randomNetworks$A
P[[2]] = randomNetworks$B
#     edgeList = lapply(P, FUN=function(x){
#       which(x[upper.tri(x)]!=0)
#     })
#
#     sum(edgeList[[1]] %in% edgeList[[2]])
edgeVal = lapply(P, FUN=function(x){
t = x[unlist(randomNetworks$id), unlist(randomNetworks$id)][upper.tri(x)]
t[which(t!=0)]#[randomNetworks$id]
})
SparseSim = sum(edgeVal[[1]]==edgeVal[[2]])/length(edgeVal[[1]])
### now simulate data:
D = NULL
for (j in 1:2){
DpopJ = vector("list", subNum)
for (k in 1:subNum){
DpopJ[[k]] = mvrnorm(n = n, mu = rep(0, p), P[[j]])
}
D = append(D, DpopJ)
}
D = lapply(D, scale)
lamMOG= regChoice(D=D, lambda = c(.15, .2), K=c(1,2,3), SampleNo = 10, Ptype="Classic")
lamMOG
X = PenVanillaEM(D = D, K = lamMOG$Kval, lambda = lamMOG$lamVal, Ptype = "Classic", tol = 0.001)
if (lamMOG$Kval>1){
iter = 0
while(X$iter==0 & iter < 5){
#       print("hi")
X = PenVanillaEM(D = D, K = lamMOG$Kval, lambda = lamMOG$lamVal, Ptype = "Classic", tol = 0.001)
iter = iter+1
}
}
X$C
trueClust = c(rep(1,subNum), rep(2, subNum))
trueEdges = c()
reportedEdges = c()
for (k in 1:length(X$SubjectTheta)){
# get estimated cluster for kth subject:
P_Est_k = X$SubjectTheta[[k]]#X$Theta[,, which.max(X$Z[k,])]
#     getPerformance(Est = P_Est_k, True = P[[trueClust[k]]])
trueEdges = c(trueEdges, 1*(P[[trueClust[k]]][upper.tri(P_Est_k)]!=0))
reportedEdges = c(reportedEdges, 1*(P_Est_k[upper.tri(P_Est_k)]!=0))
}
res = HMeasure(true.class = trueEdges, scores = reportedEdges )
ResultsMoG$SimID[i] = i
ResultsMoG$TP[i] = res$metrics$TP
ResultsMoG$FP[i] = res$metrics$FP
ResultsMoG$Sens[i] = res$metrics$TPR
ResultsMoG$Spec[i] = res$metrics$TN/(res$metrics$FP + res$metrics$TN)
ResultsMoG$FDR[i] = res$metrics$FPR
ResultsMoG$TPR[i] = res$metrics$TPR
ResultsMoG$AUC[i] = res$metrics$AUC
meanKL = 0
for (j in 1:length(D)){
meanKL = meanKL + KLDist(SigmaTrue = P[[trueClust[j]]], PresEst = X$SubjectTheta[[j]])
}
ResultsMoG$meanKL[i] = (meanKL)/length(D)
ResultsMoG$Kchoice[i] = lamMOG$Kval
ResultsMoG[1,]
lamMOG= regChoice(D=D, lambda = c(.1, .15), K=c(1,2,3), SampleNo = 10, Ptype="Classic")
lamMOG
X = PenVanillaEM(D = D, K = lamMOG$Kval, lambda = lamMOG$lamVal, Ptype = "Classic", tol = 0.001)
if (lamMOG$Kval>1){
iter = 0
while(X$iter==0 & iter < 5){
#       print("hi")
X = PenVanillaEM(D = D, K = lamMOG$Kval, lambda = lamMOG$lamVal, Ptype = "Classic", tol = 0.001)
iter = iter+1
}
}
# get performance for the MoG model:
trueClust = c(rep(1,subNum), rep(2, subNum))
trueEdges = c()
reportedEdges = c()
for (k in 1:length(X$SubjectTheta)){
# get estimated cluster for kth subject:
P_Est_k = X$SubjectTheta[[k]]#X$Theta[,, which.max(X$Z[k,])]
#     getPerformance(Est = P_Est_k, True = P[[trueClust[k]]])
trueEdges = c(trueEdges, 1*(P[[trueClust[k]]][upper.tri(P_Est_k)]!=0))
reportedEdges = c(reportedEdges, 1*(P_Est_k[upper.tri(P_Est_k)]!=0))
}
res = HMeasure(true.class = trueEdges, scores = reportedEdges )
ResultsMoG$SimID[i] = i
ResultsMoG$TP[i] = res$metrics$TP
ResultsMoG$FP[i] = res$metrics$FP
ResultsMoG$Sens[i] = res$metrics$TPR
ResultsMoG$Spec[i] = res$metrics$TN/(res$metrics$FP + res$metrics$TN)
ResultsMoG$FDR[i] = res$metrics$FPR
ResultsMoG$TPR[i] = res$metrics$TPR
ResultsMoG$AUC[i] = res$metrics$AUC
meanKL = 0
for (j in 1:length(D)){
meanKL = meanKL + KLDist(SigmaTrue = P[[trueClust[j]]], PresEst = X$SubjectTheta[[j]])
}
ResultsMoG$meanKL[i] = (meanKL)/length(D)
ResultsMoG$Kchoice[i] = lamMOG$Kval
ResultsMoG[1,]
lamMOG= regChoice(D=D, lambda = c(.15, .175), K=c(1,2,3), SampleNo = 10, Ptype="Classic")
lamMOG
X = PenVanillaEM(D = D, K = lamMOG$Kval, lambda = lamMOG$lamVal, Ptype = "Classic", tol = 0.001)
if (lamMOG$Kval>1){
iter = 0
while(X$iter==0 & iter < 5){
#       print("hi")
X = PenVanillaEM(D = D, K = lamMOG$Kval, lambda = lamMOG$lamVal, Ptype = "Classic", tol = 0.001)
iter = iter+1
}
}
# get performance for the MoG model:
trueClust = c(rep(1,subNum), rep(2, subNum))
trueEdges = c()
reportedEdges = c()
for (k in 1:length(X$SubjectTheta)){
# get estimated cluster for kth subject:
P_Est_k = X$SubjectTheta[[k]]#X$Theta[,, which.max(X$Z[k,])]
#     getPerformance(Est = P_Est_k, True = P[[trueClust[k]]])
trueEdges = c(trueEdges, 1*(P[[trueClust[k]]][upper.tri(P_Est_k)]!=0))
reportedEdges = c(reportedEdges, 1*(P_Est_k[upper.tri(P_Est_k)]!=0))
}
res = HMeasure(true.class = trueEdges, scores = reportedEdges )
ResultsMoG$SimID[i] = i
ResultsMoG$TP[i] = res$metrics$TP
ResultsMoG$FP[i] = res$metrics$FP
ResultsMoG$Sens[i] = res$metrics$TPR
ResultsMoG$Spec[i] = res$metrics$TN/(res$metrics$FP + res$metrics$TN)
ResultsMoG$FDR[i] = res$metrics$FPR
ResultsMoG$TPR[i] = res$metrics$TPR
ResultsMoG$AUC[i] = res$metrics$AUC
meanKL = 0
for (j in 1:length(D)){
meanKL = meanKL + KLDist(SigmaTrue = P[[trueClust[j]]], PresEst = X$SubjectTheta[[j]])
}
ResultsMoG$meanKL[i] = (meanKL)/length(D)
ResultsMoG$Kchoice[i] = lamMOG$Kval
ResultsMoG[1,]
D_FLG = array(0, c(n,p,2*subNum))
for (k in 1:(2*subNum)){
D_FLG[,,k] = D[[k]]
}
lamFGL = regChoiceJGL(D = D_FLG, l1=c(.05, .1), l2 = c(.2, .3))
X = FGL(data = D_FLG, l1 = lamFGL$l1, l2 = lamFGL$l2, tol = 0.001)
trueEdges = c()
reportedEdges = c()
#     for (k in 1:(2*subNum)){
#     # get estimated cluster for kth subject:
#     P_Est_k = X$theta[[k]]
#
#     trueEdges = c(trueEdges, 1*(P[[trueClust[k]]][upper.tri(P_Est_k)]!=0))
#     reportedEdges = c(reportedEdges, 1*(P_Est_k[upper.tri(P_Est_k)]!=0))
#
#     }
for (k in 1:(2*subNum)){
# get estimated cluster for kth subject:
P_Est_k = X$Z[,,k]
trueEdges = c(trueEdges, 1*(P[[trueClust[k]]][upper.tri(P_Est_k)]!=0))
reportedEdges = c(reportedEdges, 1*(P_Est_k[upper.tri(P_Est_k)]!=0))
#
}
res = HMeasure(true.class = trueEdges, scores = reportedEdges )
ResultsFGL$SimID[i] = i
ResultsFGL$TP[i] = res$metrics$TP
ResultsFGL$FP[i] = res$metrics$FP
ResultsFGL$Sens[i] = res$metrics$TPR
ResultsFGL$Spec[i] = res$metrics$TN/(res$metrics$FP + res$metrics$TN)
ResultsFGL$FDR[i] = res$metrics$FPR
ResultsFGL$TPR[i] = res$metrics$TPR
ResultsFGL$AUC[i] = res$metrics$AUC
meanKL = 0
for (j in 1:length(D)){
meanKL = meanKL + KLDist(SigmaTrue = P[[trueClust[j]]], PresEst = X$Z[,,j])
#       meanKL = meanKL + KLDist(SigmaTrue = P[[trueClust[j]]], PresEst = X$theta[[j]])
}
ResultsFGL$meanKL[i] = (meanKL)/length(D)
cat("FGL done\n")
ResultsFGL[1,]
ResultsMoG[1,]
lamGGL = regChoiceGGL(D = D_FLG, l1 = c(0.05, 0.1), l2=c( .25, .3))
X = JGL(Y = D, penalty = "group", lambda1 = lamGGL$l1, lambda2 = lamGGL$l2, return.whole.theta = TRUE)
trueEdges = c()
reportedEdges = c()
for (k in 1:(2*subNum)){
# get estimated cluster for kth subject:
P_Est_k = X$theta[[k]]
trueEdges = c(trueEdges, 1*(P[[trueClust[k]]][upper.tri(P_Est_k)]!=0))
reportedEdges = c(reportedEdges, 1*(P_Est_k[upper.tri(P_Est_k)]!=0))
}
res = HMeasure(true.class = trueEdges, scores = reportedEdges )
ResultsGGL$SimID[i] = i
ResultsGGL$TP[i] = res$metrics$TP
ResultsGGL$FP[i] = res$metrics$FP
ResultsGGL$Sens[i] = res$metrics$TPR
ResultsGGL$Spec[i] = res$metrics$TN/(res$metrics$FP + res$metrics$TN)
ResultsGGL$FDR[i] = res$metrics$FPR
ResultsGGL$TPR[i] = res$metrics$TPR
ResultsGGL$AUC[i] = res$metrics$AUC
meanKL = 0
for (j in 1:length(D)){
meanKL = meanKL + KLDist(SigmaTrue = P[[trueClust[j]]], PresEst = X$theta[[j]])
}
ResultsGGL$meanKL[i] = (meanKL)/length(D)
ResultsGGL[1,]
trueEdges = c()
reportedEdges = c()
meanKL = 0
for (k in 1:(2*subNum)){
# choose reg parameter first:
lamGlassoK = regChoiceGlasso(D[[k]], l1=c(.05, .1, .2))
# get estimated cluster for kth subject:
P_Est_k = glasso(cov(D[[k]]), rho=lamGlassoK)$wi
trueEdges = c(trueEdges, 1*(P[[trueClust[k]]][upper.tri(P_Est_k)]!=0))
reportedEdges = c(reportedEdges, 1*(P_Est_k[upper.tri(P_Est_k)]!=0))
meanKL = meanKL + KLDist(SigmaTrue = P[[trueClust[k]]], PresEst = P_Est_k)
}
res = HMeasure(true.class = trueEdges, scores = reportedEdges )
ResultsGL$SimID[i] = i
ResultsGL$TP[i] = res$metrics$TP
ResultsGL$FP[i] = res$metrics$FP
ResultsGL$Sens[i] = res$metrics$TPR
ResultsGL$Spec[i] = res$metrics$TN/(res$metrics$FP + res$metrics$TN)
ResultsGL$FDR[i] = res$metrics$FPR
ResultsGL$TPR[i] = res$metrics$TPR
ResultsGL$AUC[i] = res$metrics$AUC
ResultsGL$meanKL[i] = meanKL/length(D)
cat("Results (AUC) of iteration:", i, "...\n")
cat( ResultsMoG$AUC[i],  ResultsFGL$AUC[i],  ResultsGGL$AUC[i], ResultsGL$AUC[i], "\n")
#   cat("Mixture of Gaussians:", ResultsMoG$AUC[i],  "\n")
ResultsGL[1,]
rm(list=ls())
gc()
library(MASS)
library(igraph)
library(mclust)
library(JGL) # might be better idea to use my own implementation for FGL
source('/media/1401-1FFE/Documents/Model Based Clustering/Code/PenalisedVanillaMoG_02.R')
source('/media/1401-1FFE/Documents/Model Based Clustering/Code/NetworkSim.R')
source('/media/1401-1FFE/Documents/Model Based Clustering/Simulations/BreakJGL/Helper_01.R')
source('/media/1401-1FFE/Documents/Model Based Clustering/Code/FGL/FGL.R')
source('/media/1401-1FFE/Documents/Model Based Clustering/Simulations/BreakJGL/SimilarNetworkGeneration_01.R')
## ---- SIMULATION 1 ---- ##
i=1
p=10
rho=0
set.seed(i)
randomNetworks = Gen2SharedNetworks(p = p, rho = rho, sparsity = .2)
names(randomNetworks)
P = vector("list", 2)
P[[1]] = randomNetworks$y
P[[2]] = randomNetworks$y2
### now simulate data:
D = NULL
for (j in 1:popNum){
DpopJ = vector("list", subNum)
for (k in 1:subNum){
DpopJ[[k]] = mvrnorm(n = n, mu = rep(0, p), P[[j]])
}
D = append(D, DpopJ)
}
D = lapply(D, scale)
popNum=5
popNum=2
subNum=5
set.seed(i)
randomNetworks = Gen2SharedNetworks(p = p, rho = rho, sparsity = .2)
P = vector("list", 2)
P[[1]] = randomNetworks$y
P[[2]] = randomNetworks$y2
### now simulate data:
D = NULL
for (j in 1:popNum){
DpopJ = vector("list", subNum)
for (k in 1:subNum){
DpopJ[[k]] = mvrnorm(n = n, mu = rep(0, p), P[[j]])
}
D = append(D, DpopJ)
}
D = lapply(D, scale)
n = 20
set.seed(i)
randomNetworks = Gen2SharedNetworks(p = p, rho = rho, sparsity = .2)
P = vector("list", 2)
P[[1]] = randomNetworks$y
P[[2]] = randomNetworks$y2
### now simulate data:
D = NULL
for (j in 1:popNum){
DpopJ = vector("list", subNum)
for (k in 1:subNum){
DpopJ[[k]] = mvrnorm(n = n, mu = rep(0, p), P[[j]])
}
D = append(D, DpopJ)
}
D = lapply(D, scale)
length(D)
dim(D[[1]])
dim(D[1])
setwd('/media/1401-1FFE/Documents/Model Based Clustering/OMG/PythonCode/Sample Data/')
length(D)
?write.csv
paste0('Subject_', i, '_.csv')
setwd('/media/1401-1FFE/Documents/Model Based Clustering/OMG/PythonCode/Sample Data/')
for (i in 1:length(D)){
write.csv(D[[i]], paste0('Subject_', i, '_.csv'))
setwd('/media/1401-1FFE/Documents/Model Based Clustering/OMG/PythonCode/Sample Data/')
for (i in 1:length(D)){
write.csv(D[[i]], paste0('Subject_', i, '_.csv'), row.names=FALSE)
}
length(D)
cov(D[[1]])
glasso(cov(D[[1]]), rho=0.05)$wi
?glasso
